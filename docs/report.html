<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="George Lee, Emil Bennewitz, Charles Gendreau">
<meta name="dcterms.date" content="2023-12-24">

<title>Main Project</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="report_files/libs/clipboard/clipboard.min.js"></script>
<script src="report_files/libs/quarto-html/quarto.js"></script>
<script src="report_files/libs/quarto-html/popper.min.js"></script>
<script src="report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="report_files/libs/quarto-html/anchor.min.js"></script>
<link href="report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#the-em-algorithm" id="toc-the-em-algorithm" class="nav-link" data-scroll-target="#the-em-algorithm"><span class="header-section-number">2</span> The EM Algorithm</a>
  <ul class="collapse">
  <li><a href="#general-setup" id="toc-general-setup" class="nav-link" data-scroll-target="#general-setup"><span class="header-section-number">2.1</span> General setup</a></li>
  <li><a href="#em-for-a-multivariate-normal-with-missing-data" id="toc-em-for-a-multivariate-normal-with-missing-data" class="nav-link" data-scroll-target="#em-for-a-multivariate-normal-with-missing-data"><span class="header-section-number">2.2</span> EM for a multivariate normal with missing data</a></li>
  <li><a href="#e-step" id="toc-e-step" class="nav-link" data-scroll-target="#e-step"><span class="header-section-number">2.3</span> E-step</a></li>
  </ul></li>
  <li><a href="#analyzing-the-suitability-of-the-em-algorithm-for-parameter-inference-in-the-multivariate-gaussian-setting." id="toc-analyzing-the-suitability-of-the-em-algorithm-for-parameter-inference-in-the-multivariate-gaussian-setting." class="nav-link" data-scroll-target="#analyzing-the-suitability-of-the-em-algorithm-for-parameter-inference-in-the-multivariate-gaussian-setting."><span class="header-section-number">3</span> Analyzing the suitability of the EM algorithm for parameter inference in the multivariate Gaussian setting.</a>
  <ul class="collapse">
  <li><a href="#different-patterns-of-missingness" id="toc-different-patterns-of-missingness" class="nav-link" data-scroll-target="#different-patterns-of-missingness"><span class="header-section-number">3.1</span> Different patterns of missingness</a></li>
  <li><a href="#missing-completely-at-random-mcar" id="toc-missing-completely-at-random-mcar" class="nav-link" data-scroll-target="#missing-completely-at-random-mcar"><span class="header-section-number">3.2</span> Missing completely at random (MCAR)</a></li>
  <li><a href="#missing-at-random-mar" id="toc-missing-at-random-mar" class="nav-link" data-scroll-target="#missing-at-random-mar"><span class="header-section-number">3.3</span> Missing at random (MAR)</a></li>
  <li><a href="#missing-not-at-random-mnar" id="toc-missing-not-at-random-mnar" class="nav-link" data-scroll-target="#missing-not-at-random-mnar"><span class="header-section-number">3.4</span> Missing not at random (MNAR)</a></li>
  </ul></li>
  <li><a href="#first-results" id="toc-first-results" class="nav-link" data-scroll-target="#first-results"><span class="header-section-number">4</span> First results</a>
  <ul class="collapse">
  <li><a href="#other-imputation-methods" id="toc-other-imputation-methods" class="nav-link" data-scroll-target="#other-imputation-methods"><span class="header-section-number">4.1</span> Other imputation methods</a></li>
  <li><a href="#comparison-metrics" id="toc-comparison-metrics" class="nav-link" data-scroll-target="#comparison-metrics"><span class="header-section-number">4.2</span> Comparison metrics</a></li>
  </ul></li>
  <li><a href="#the-impact-of-different-types-of-covariance-matrices-on-the-em-algorithm" id="toc-the-impact-of-different-types-of-covariance-matrices-on-the-em-algorithm" class="nav-link" data-scroll-target="#the-impact-of-different-types-of-covariance-matrices-on-the-em-algorithm"><span class="header-section-number">5</span> The impact of different types of covariance matrices on the EM algorithm</a></li>
  <li><a href="#non-gaussian-distributions" id="toc-non-gaussian-distributions" class="nav-link" data-scroll-target="#non-gaussian-distributions"><span class="header-section-number">6</span> Non-Gaussian Distributions</a>
  <ul class="collapse">
  <li><a href="#real-data-with-no-distribution-assumption" id="toc-real-data-with-no-distribution-assumption" class="nav-link" data-scroll-target="#real-data-with-no-distribution-assumption"><span class="header-section-number">6.1</span> Real Data with no distribution assumption</a></li>
  <li><a href="#students-t-data" id="toc-students-t-data" class="nav-link" data-scroll-target="#students-t-data"><span class="header-section-number">6.2</span> Student’s-t Data</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">7</span> Conclusion</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix"><span class="header-section-number">8</span> Appendix</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="report.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><strong>Main Project</strong></h1>
<p class="subtitle lead"><em>EM algorithm for Gaussian missing data</em></p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>George Lee, Emil Bennewitz, Charles Gendreau </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 24, 2023</p>
    </div>
  </div>
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>The EM algorithm is a valuable statistical tool for performing inference on incomplete data. It can also be used for imputation. We will investigate under which conditions its implementation for incomplete Gaussian data performs best. We will compare with other popular imputation methods, and look at how the EM algorithm matches up in terms of imputation and inference.</p>
  </div>
</div>

</header>

<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Missing data points are common in applications and real life data sets, especially in Economics, Political and Social Sciences. This is either because some entities do not want to share data, or because they are not collected. Technical reasons could also cause missing data, such as machine failures or non-responses. Missing data can significantly affect the conclusions drawn from a data set, and uncertainty about the conclusions increases as the proportion of missing data increases. Various methods allow to perform inference on incomplete data, among which there is the Expectation-Maximization (EM) algorithm. The EM also permits replacing the missing data points by estimates (imputation). We will compare the EM to other imputation methods, and also compare EM inference to inference performed on imputed data (using other imputation methods).</p>
<p>In this report, we will focus on the EM algorithm for Gaussian data. We will compare it with other methods of imputation, such as Median Imputation, K Nearest Neighbors (KNN), and Iterative Imputation (also known as Multiple Imputation by Chained Equations, MICE). We will also compare the performance over different missing data mechanisms. Finally, we will see how much the distributional assumptions required by the EM algorithm can affect the imputation and inference in a real life data set (and briefly mention synthetic Student’s-t distributed data).</p>
</section>
<section id="the-em-algorithm" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="the-em-algorithm"><span class="header-section-number">2</span> The EM Algorithm</h2>
<p>In this section, we present the EM-algorithm in general <span class="citation" data-cites="10.1111/j.2517-6161.1977.tb01600.x">(<a href="#ref-10.1111/j.2517-6161.1977.tb01600.x" role="doc-biblioref">Dempster et al., 2018</a>)</span>, and then present the specifics of implementing the algorithm in the special case of a multivariate normal with missing data.</p>
<section id="general-setup" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="general-setup"><span class="header-section-number">2.1</span> General setup</h3>
<p>Let’s say we observe some data from a survey with lots of questions. This could be in a regression setup, for example where we use the answers from some questions to predict the answer of a question of interest (“Have you ever had a heart attack?”, for example), but it could also be something else and we are just interested in estimating some parameters of the data. This could be the mean vector and covariance matrix. It’s very common for participants not to be asked all the questions.</p>
<p>Mathematically, the responses to the survey containing <span class="math inline">p</span> questions correspond to a random vector</p>
<p><span class="math display">
\overset{\to}{X}=\left(X_1,\dots,X_p\right)\sim F_{\overset{\to}{X}}
</span></p>
<p>with some distribution. Some of the <span class="math inline">X_i</span> are unknown, missing. This makes it harder to infer the parameters of interest of the data.</p>
<p>We gather all the independent observations (in the survey example, one observation is a, potentially partially, filled out survey) in the rows of a <span class="math inline">(N,p)</span> dimensional data matrix <span class="math inline">\mathbf{X}</span> .</p>
<p>Typically, without the presence of missing data, we model the data as coming from a parametric distribution with density <span class="math inline">f(x|\theta)</span> and estimate the parameters by maximizing the (log)likelihood. In the presence of missing data, we cannot calculate the complete loglikelihood <span class="math inline">\ell_{\text{comp}}(\theta)</span>. We have to work with the likelihood of the observed data, which we denote (as a random variable) <span class="math inline">X_{\text{obs}}</span> ; this corresponds to all the entries of the data matrix which are not missing. We know the realization of this random variable is <span class="math inline">x_{\text{obs}}</span>. The random variable <span class="math inline">X_{\text{miss}}</span> corresponds to the missing entries.</p>
<p>Explicitly, the write the observed and complete loglikelihoods as below.</p>
<p><span class="math display">
\ell_{\text{comp}}(\theta):=\log f(X_{\text{obs}},X_{\text{miss}}|\theta)
</span></p>
<p><span class="math display">
\ell_{\text{obs}}(\theta):=\log f(X_{\text{miss}}|\theta,X_{\text{obs}}=x_{\text{obs}})
</span></p>
<p>Typically, the complete loglikelihood is very nice to optimize, but we cannot work with it because we don’t know the missing data. The observed loglikelihood is not nice to optimize.</p>
<p>The EM-algorithm is an iterative algorithm that allows us to optimize the observed loglikelihood. It does this by optimizing the expected complete loglikelihood <span class="math inline">\mathbb{E_{\theta_0}}\left[\ell_{\text{comp}}(\theta)\right|X_{\text{obs}}]</span> with respect to <span class="math inline">\theta</span>. The expectation is taken with respect to the conditional distribution of the missing data given the observed data and an initial estimate <span class="math inline">\theta_0</span> of <span class="math inline">\theta</span>.</p>
<p>The EM algorithm is in two steps. First, choose an initial estimate <span class="math inline">\theta_0</span> of <span class="math inline">\theta</span>. Then, iterate the following two steps until convergence:</p>
<ol type="1">
<li>E-step: Calculate <span class="math inline">\mathbb{E}_{\theta_{l-1}}[\ell_{\text{comp}}(\theta)|X_{\text{obs}}]=:Q(\theta,\theta_{l-1})</span></li>
<li>M-step: Find <span class="math inline">\theta_l</span> that maximizes <span class="math inline">Q(\theta,\theta_{l-1})</span>.</li>
</ol>
<p>One can show that the sequence of thetas keeps augmenting <span class="math inline">\ell_{\text{obs}}(\theta)</span>, which is good for optimization.</p>
</section>
<section id="em-for-a-multivariate-normal-with-missing-data" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="em-for-a-multivariate-normal-with-missing-data"><span class="header-section-number">2.2</span> EM for a multivariate normal with missing data</h3>
<p>This section essentially follows the course notes <span class="citation" data-cites="math_517_website">(<a href="#ref-math_517_website" role="doc-biblioref">Math 517 Course, 2023</a>)</span>.</p>
<p>Now suppose the random vector <span class="math display">
\overset{\to}{X}=\left(X_1,\dots,X_p\right)\sim N_p(\mu,\Sigma)
</span></p>
<p>is multivariate normal. The i-th row of the data matrix <span class="math inline">\mathbf{X}</span> is the realization <span class="math inline">x^{(i)}</span> of the random vector <span class="math inline">\overset{\to}{X}^{(i)}</span>. We shall omit the arrow for simplicity of notation.</p>
<p>Then we obtain <span class="math display">
\ell_{\text{comp}}(\mu,\Sigma)=\sum_{n=1}^n\log f_{\overset{\to}{X}}(X^{(n)}|\mu,\Sigma)\equiv -N/2 \log(\det(\Sigma))+\frac{1}{2}\sum_{n=1}^N (x^{(n)}-\mu)^T\Sigma^{-1}(x^{(n)}-\mu)
</span></p>
<p>Given that the summands are scalar, we can take the trace of the summands and use the properties of the trace, so that when we take the expectation conditional on the observed data, we get <span class="math display">
Q((\mu,\Sigma);(\mu_{l-1},\Sigma_{l-1}))\equiv -N/2 \log(\det(\Sigma)) +\frac{1}{2}\sum_{n=1}^N tr\left\{\mathbb{E}_{(\mu_{l-1},\Sigma_{l-1})}\left[(X^{(n)}-\mu)(X^{(n)}-\mu)^T|x_{\text{obs}}\right]\Sigma^{-1}\right\}
</span></p>
</section>
<section id="e-step" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="e-step"><span class="header-section-number">2.3</span> E-step</h3>
<p>We denote <span class="math inline">\hat{x}^{(n)(l)}=\mathbb{E}_{(\mu_{l},\Sigma_{l})}\left[ X^{(n)}|X_{\text{obs}}=x_{\text{obs}} \right]</span>. Consider <span class="math inline">\hat{X}^{(n)(l)}=\mathbb{E}_{(\mu_{l},\Sigma_{l})}\left[ X^{(n)}|X_{\text{obs}}\right]</span> the corresponding random variable.<br>
It’s a classical result that if we have a multivariate Gaussian <span class="math inline">Z=(Z_1,Z_2)</span>, with mean <span class="math inline">\begin{pmatrix}\mu_1\\\mu_2 \end{pmatrix}</span> and covariance <span class="math inline">\begin{pmatrix} \Sigma_{11}&amp;&amp;\Sigma_{12} \\ \Sigma_{21}&amp;&amp;\Sigma_{22} \end{pmatrix}</span>, then <span class="math inline">Z_2|Z_1=z_1</span> is a random variable with a <span class="math inline">N(\mu_2+\Sigma_{21}\Sigma_{11}^{-1}(z_1-\mu_1),\Sigma_{2.1})</span> distribution, where <span class="math inline">\Sigma_{2.1}=\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}</span>.</p>
<p>Hence we can calculate <span class="math inline">\hat{x}^{(n)(l)}</span>.</p>
<p>We can then write <span class="math display">
\begin{aligned}
(X^{(n)}-\mu)(X^{(n)}-\mu)^T &amp;= \left((X^{(n)}-\hat{x}^{(n)(l)})+(\hat{x}^{(n)(l)}-\mu)\right)\left((X^{(n)}-\hat{x}^{(n)(l)})+(\hat{x}^{(n)(l)}-\mu)\right)^T \\
&amp;= (X^{(n)}-\hat{x}^{(n)(l)})(X^{(n)}-\hat{x}^{(n)(l)})^T+(\hat{x}^{(n)(l)}-\mu)(\hat{x}^{(n)(l)}-\mu)^T \\
&amp;\quad+(X^{(n)}-\hat{x}^{(n)(l)})(\hat{x}^{(n)(l)}-\mu)^T+(\hat{x}^{(n)(l)}-\mu)(X^{(n)}-\hat{x}^{(n)(l)})^T
\end{aligned}
</span></p>
<p>Finally, taking the conditional expectation we obtain (as in the expression for Q, so with <span class="math inline">l-1</span>)</p>
<p><span class="math display">
\begin{aligned}
\mathbb{E}_{(\mu_{l-1},\Sigma_{l-1})}\left[(X^{(n)}-\mu)(X^{(n)}-\mu)^T|x_{\text{obs}}\right] &amp;= \mathbb{E}_{(\mu_{l-1},\Sigma_{l-1})}\left[(X^{(n)}-\hat{x}^{(n)(l-1)})(X^{(n)}-\hat{x}^{(n)(l-1)})^T|x_{\text{obs}}\right] \\
&amp;+ (\hat{x}^{(n)(l-1)}-\mu)(\hat{x}^{(n)(l-1)}-\mu)^T
\end{aligned}
</span></p>
<p>Note that the first term is the conditional covariance of <span class="math inline">X^{(n)}</span> given the realization <span class="math inline">X_{\text{obs}}=x_{\text{obs}}</span>. Let’s denote this matrix <span class="math inline">C^{(n)(l)}</span>.</p>
<p>We can write</p>
<p><span class="math display">
\begin{aligned}
Q((\mu,\Sigma);(\mu_{l-1},\Sigma_{l-1})) &amp;\equiv \underbrace{-\frac{N}{2} \log(\det(\Sigma)) + \frac{1}{2}\sum_{n=1}^N \text{tr}\left\{(x^{(n)(l-1)}-\mu)(x^{(n)(l-1)}-\mu)^T \Sigma^{-1}\right\}}_{(1)} \\
&amp;+ \underbrace{\frac{1}{2}\sum_{n=1}^N \text{tr}\left\{C^{(n)(l-1)}\Sigma^{-1}\right\}}_{(2)}
\end{aligned}
</span></p>
<p>We recognize the first term as a regular Gaussian loglikelihood. The second does not depend on <span class="math inline">\mu</span>. We know that the <span class="math inline">\mu</span> which maximizes <span class="math inline">(1)</span> (and hence <span class="math inline">(1)+(2)</span>) doesn’t depend on <span class="math inline">\Sigma</span>, and is given by <span class="math inline">\mu^{(l)}=\frac{1}{N}\sum_{n=1}^N(x^{(n)(l-1)})^T</span>. To find <span class="math inline">\arg\max\Sigma</span>, we use the property (which is not difficult to show) <span class="math inline">\frac{\partial}{\partial A}tr(AB)=B^T</span>. We also use the fact that <span class="math inline">\frac{\partial \det(A)}{\partial A}=\det (A)A</span>. Differentiating with respect to <span class="math inline">\Sigma</span>, we hence obtain <span class="math display">
-\frac{N}{2} \Sigma+\frac{1}{2}\sum_{n=1}^N(x^{(n)(l-1)}-\mu)(x^{(n)(l-1)}-\mu)^T+\frac{1}{2}\sum_{n=1}^N C^{(n)(l-1)}.
</span></p>
<p>Setting the derivative to zero, we obtain the update rule for <span class="math inline">\Sigma</span>:</p>
<p><span class="math display">
\Sigma^{(l)}=\frac{1}{N}\sum_{n=1}^N(\hat{x}^{(n)(l-1)}-\mu^{(l)})(\hat{x}^{(n)(l-1)}-\mu^{(l)})^T+ C^{(n)(l-1)}
</span></p>
<p>is the matrix which maximizes <span class="math inline">Q((\mu,\Sigma);(\mu_{l-1},\Sigma_{l-1}))</span>.</p>
<p>So to resume, an iteration is as follows:</p>
<ol type="1">
<li><p>Starting from current estimates <span class="math inline">\mu^{(l-1)}</span> and <span class="math inline">\Sigma^{(l-1)}</span>, calculate <span class="math inline">\hat{x}^{(n)(l-1)}=\mathbb{E}_{(\mu_{l-1},\Sigma_{l-1})}\left[ X^{(n)}|x_{\text{obs}} \right]</span>.</p></li>
<li><p>Use this vector to calculate the updated parameter estimates <span class="math inline">\mu^{(l)}</span> and <span class="math inline">\Sigma^{(l)}</span> according to the formulas above.</p></li>
</ol>
<p>We remark that this is <span class="math inline">\textit{not}</span> the same as imputing the values of <span class="math inline">\hat{x}^{(n)(l-1)}</span> into the corresponding missing values of the data matrix and then taking the MLEs of the complete Gaussian (empirical mean and covariance) to update the parameters. However, we <span class="math inline">\textit{do}</span> need to calculate <span class="math inline">\hat{x}^{(n)(l-1)}</span> at each iteration. This implies that, as a by-product of the algorithm, at the final iteration of the algorithm, we obtain <span class="math inline">\hat{x}^{(n)(L-1)}</span>, which is a good estimate of the missing values of the data matrix. In this sense, the EM algorithm can also be used for imputation, not just parameter estimation. How well the algorithm imputes the data is also basically simply another measure of how well it has estimated the parameters.</p>
</section>
</section>
<section id="analyzing-the-suitability-of-the-em-algorithm-for-parameter-inference-in-the-multivariate-gaussian-setting." class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="analyzing-the-suitability-of-the-em-algorithm-for-parameter-inference-in-the-multivariate-gaussian-setting."><span class="header-section-number">3</span> Analyzing the suitability of the EM algorithm for parameter inference in the multivariate Gaussian setting.</h2>
<section id="different-patterns-of-missingness" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="different-patterns-of-missingness"><span class="header-section-number">3.1</span> Different patterns of missingness</h3>
<p>We now discuss the different patterns of missingness that can occur in a dataset.</p>
<p>Consider a data set <span class="math inline">X \in \Omega_1 \times \cdots \times \Omega_p</span> which is a concatenation of <span class="math inline">d</span> columns <span class="math inline">X_j \in \Omega_j</span>, where <span class="math inline">\Omega_j</span> is the support of the variable <span class="math inline">X_j</span> which is of dimension <span class="math inline">dim(\Omega_j) = n</span>, representing the number of observations. This gives us a dataset of dimension <span class="math inline">n \times d</span>. For example, one could have <span class="math inline">\Omega_j = \mathbb{R}^n, \mathbb{Z}^n, \mathcal{S}^n</span>, where <span class="math inline">\mathcal{S} = \{s_1, ...., s_k\}</span>, for some quantitative or qualitative values <span class="math inline">s_i</span>, <span class="math inline">i=1..., k; ~ k \in \mathbb{N}</span></p>
<p>Consider the response matrix <span class="math inline">R\in \{0,1\}^{n \times d}</span> defined by <span class="math inline">R_{ij} = 1</span> if <span class="math inline">X_{ij}</span> is observed and <span class="math inline">0</span> otherwise. We now partition the data matrix <span class="math inline">X = \{X^{obs}, X^{miss}\}</span>, where <span class="math inline">X^{obs}</span> and <span class="math inline">X^{miss}</span> are the matrices containing the observed and missing values: <span class="math inline">X^{obs}_{ij} = X_{ij} I_{\{R_{ij}=1\}}</span>, <span class="math inline">X^{miss}_{ij} = X_{ij} I_{\{R_{ij}=0\}}</span>.</p>
<p>In the code, the matrix <span class="math inline">R</span> is seen as a boolean tensor called a <em>mask</em> and has value <em>True</em> at position <span class="math inline">(i,j)</span>, whenever <span class="math inline">X_{ij}</span>, the i-th observation of the j-th variable is observed. In order to generate missing data, one has to generate a mask <span class="math inline">R</span> and then apply it to the complete data <span class="math inline">X</span>. If one could easily generate a mask from <span class="math inline">n \times d</span> independent Bernouilli samples. To generate a mask <span class="math inline">R</span> in a non-independent way, one can use a logistic model with a sigmoid function. This consists in using the sigmoid function <span class="math display">\sigma(z) = \frac{1}{1 + e^{-z}}.</span> One will generate from a d-dimensional standard normal distribution weights <span class="math inline">W</span> and find a vector of intercepts <span class="math inline">b \in \mathbb{R}^n</span> such that <span class="math inline">\sigma(WX + b) \in (0,1)^n</span>. We can then define the probability <span class="math display">\mathbb{P}(R_{ij} = 1 \mid X) = \sigma((WX)_i + b_i)</span> and generate a mask <span class="math inline">R</span> respecting these probabilities. Observe that the missingness <span class="math inline">R\_{ij}</span> of the i-th observation of the variable <span class="math inline">j</span> depends on the other variables <span class="math inline">X_{ij}</span>, <span class="math inline">j=1,...,d</span>.</p>
<p>There exists different types of missing data mechanisms which fall into the following categories.</p>
</section>
<section id="missing-completely-at-random-mcar" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="missing-completely-at-random-mcar"><span class="header-section-number">3.2</span> Missing completely at random (MCAR)</h3>
<p>Observations are said to be be missing completely at random (MCAR) if <span class="math inline">R \perp X</span> that is if <span class="math inline">\mathbb{P}(R \in A \mid X^{obs}, X^{miss}) = \mathbb{P}(R \in A)</span> for any <span class="math inline">A \in \sigma\{0,1\}^{n \times d}</span>. To generate such missingness, one would start form a complete data matrix <span class="math inline">X</span> and generate an independent matrix <span class="math inline">R</span>.</p>
<p>In the code, the mask corresponding to <span class="math inline">R</span> is generated independently from a <span class="math inline">n \times d</span> uniform distribution and a certain probabibility of observed values <span class="math inline">p_{obs}</span>. With such a mask, the data will have in expectation a proportion of missing data of <span class="math inline">p_{miss} = 1 - p_{obs}</span>. Clearly, here the missingness does not depend on the variables.</p>
</section>
<section id="missing-at-random-mar" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="missing-at-random-mar"><span class="header-section-number">3.3</span> Missing at random (MAR)</h3>
<p>Observations are said to be be missing at random (MAR) if <span class="math inline">R \perp X^{miss}</span> that is if <span class="math inline">\mathbb{P}(R \in A \mid X^{obs}, X^{miss}) = \mathbb{P}(R \in A \mid X^{obs})</span> for any <span class="math inline">A \in \sigma\{0,1\}^{n \times p}</span>. To generate such missingness, one would start form a complete data matrix <span class="math inline">X</span> and generate a matrix <span class="math inline">R</span> independent from <span class="math inline">X^{miss}</span> but not from the observed data <span class="math inline">X^{obs}</span>.</p>
<p>To generate such missingness, one again needs to generate a mask <span class="math inline">R</span> that depends on the observed values but not on the missing ones. To do so, we used a mask by selecting at random (uniformly) <span class="math inline">p_{obs} \cdot d</span> variables (or columns) which will have no missing values. For the other variables, we use a logistic model to generate missingness with a fixed missing probability common for each missing variables. This then gives us a MAR response matrix.</p>
</section>
<section id="missing-not-at-random-mnar" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="missing-not-at-random-mnar"><span class="header-section-number">3.4</span> Missing not at random (MNAR)</h3>
<p>If missingness is not MCAR or MAR, it is said to be missing not at random (MNAR). To generate such data, one has various options. The first one, is to consider a self-masked model which will apply the logistic model to all variables, meaning that every variable will potentially have missing values.</p>
<p>The second one is to select a certain proportion of variables which will be used as inputs for the logistic model and the remaining variables will have missing probabilities according to the logistic model. Then a MCAR mechanism is applied to the input variables. After this transformation one indeed has a dependence of the missingness of the two groups of variables and hence the mask <span class="math inline">R</span> depends on missing observations (<span class="math inline">X^{miss}</span>) and on the observed ones (<span class="math inline">X^{obs}</span>).</p>
<p>The code used to generate missing data and its description was taken from <span class="citation" data-cites="rmisstastic_generate_missing_values">(<a href="#ref-rmisstastic_generate_missing_values" role="doc-biblioref">Rmisstastic</a>)</span>.</p>
</section>
</section>
<section id="first-results" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="first-results"><span class="header-section-number">4</span> First results</h2>
<section id="other-imputation-methods" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="other-imputation-methods"><span class="header-section-number">4.1</span> Other imputation methods</h3>
<p>We started by simulating data from a 10-dimensional Gaussian with random <span class="math inline">N(0,10\cdot I_5)</span> mean. We also randomly generated a matrix A with random entries from <span class="math inline">N(0,1)</span> and chose <span class="math inline">A^T A</span> as the true covariance matrix we simulated the data from. The goal is to recover the true mean and covariance matrix from the data. We then randomly removed a certain percentage of the data according to a certain pattern of missingness, using functions from an external library for this purpose, as explained above. For each of these different missing data mechanisms, we analyzed the results.</p>
<p>We wanted to compare the results of the EM algorithm with other ways of handling missing data. As such, we will compare it to median imputation, KNN, and Iterative imputation.</p>
<p>In the realm of data imputation, median imputation stands out for its simplicity, particularly in scenarios where the data distribution is not symmetric. This method involves replacing missing values in a dataset with the median of the observed (non-missing) values within the same feature. The rationale behind this approach is grounded in the statistical properties of the median as a measure of central tendency, which, unlike the mean, is not skewed by outliers. Therefore, in distributions that are not symmetrical or when outliers are present, the median provides a more robust estimate of the central location of the data.</p>
<p>The simplicity of median imputation lies in its non-parametric nature – it doesn’t assume an underlying distribution for the data. However, this simplicity comes at the cost of certain limitations. Most notably, median imputation treats each feature independently and does not account for potential correlations between different features. This independent treatment might not be ideal in datasets where variables are interrelated. Additionally, applying median imputation extensively, especially in cases with a significant amount of missing data, can lead to an underestimation of the variability in the data and may result in biased estimates. Despite these limitations, median imputation remains a popular choice in preliminary data cleaning and exploratory data analysis, especially when dealing with non-normally distributed data or data with outliers.</p>
<p>K-Nearest Neighbors (KNN) Imputation <span class="citation" data-cites="10.1093/bioinformatics/17.6.520">(<a href="#ref-10.1093/bioinformatics/17.6.520" role="doc-biblioref">Troyanskaya et al., 2001</a>)</span> is a method employed in statistics and data science to impute missing values in datasets. This non-parametric technique assumes that the dataset contains similar data points, and these similarities can be leveraged to estimate missing values.</p>
<p>The KNN imputation method operates on the principle that the missing values of a data point can be inferred from the ‘k’ nearest points in the feature space. These nearest neighbors are determined based on the non-missing features of the data point in question. The method uses distance metrics to ascertain the closeness of data points. Commonly used distance metrics in KNN are:</p>
<ol type="1">
<li><strong>Euclidean Distance</strong>: Defined as the square root of the sum of squared differences between two points in an m-dimensional space, given by the formula: <span class="math inline">d(\mathbf{x}_i, \mathbf{x}_j) = \sqrt{\sum_{k=1}^{m}(x_{ik} - x_{jk})^2}</span></li>
<li><strong>Manhattan Distance</strong>: Calculated as the sum of the absolute differences of their coordinates, expressed as: <span class="math inline">d(\mathbf{x}_i, \mathbf{x}_j) = \sum_{k=1}^{m}|x_{ik} - x_{jk}|</span></li>
<li><strong>Minkowski Distance</strong>: A generalized form of distance, which becomes Euclidean or Manhattan distance for specific parameter values, represented as: <span class="math inline">d(\mathbf{x}_i, \mathbf{x}_j) = \left( \sum_{k=1}^{m}|x_{ik} - x_{jk}|^p \right)^{\frac{1}{p}}</span></li>
</ol>
<p>The procedure for KNN imputation includes normalization of the data to ensure uniform contribution of each feature to the distance calculations. For an observation with missing values, the algorithm computes distances to other data points considering only the non-missing features. It then identifies the ‘k’ nearest neighbors based on these distances. The missing values are imputed using a statistical measure (like mean or median) derived from the values of these neighbors.</p>
<p>While KNN imputation is versatile and adaptable to the data’s structure, it has its limitations. The method can be computationally intensive for large datasets, and its performance is sensitive to outliers. The choice of ‘k’ is critical and can significantly influence the imputation’s accuracy. In high-dimensional spaces, proximity may become less meaningful, a problem known as the curse of dimensionality.</p>
<p>We decided to use the default <span class="math inline">k=5</span> for our code.</p>
<p>KNN imputation is particularly useful in scenarios where the data’s underlying structure does not conform to linear assumptions, making it a valuable tool for handling missing data in diverse datasets.</p>
<p>Iterative Imputation in Scikit-learn, an implementation of the Multiple Imputation by Chained Equations (MICE) algorithm <span class="citation" data-cites="JSSv045i03">(<a href="#ref-JSSv045i03" role="doc-biblioref">Buuren and Groothuis-Oudshoorn, 2011</a>)</span>, is a sophisticated technique for handling missing data in datasets. This approach is particularly effective when dealing with data that are not Missing Completely At Random (MCAR) but rather Missing At Random (MAR) or Missing Not At Random (MNAR).</p>
<p>The fundamental concept of Iterative Imputation revolves around modeling each feature with missing values as a dependent variable in a regression framework. The algorithm initially fills in missing values using simple imputation methods such as mean, median, or mode. This initial imputation creates a complete but approximate dataset. The algorithm then refines these imputations iteratively, treating each feature with missing data as a dependent variable in a regression model. The other features in the dataset act as independent variables or predictors in this model.</p>
<p>Mathematically, the iterative process can be described as follows:</p>
<ol type="1">
<li><p><strong>Initial Imputation</strong>: Missing values in the dataset are imputed using a simple method (e.g., mean, median), we used median imputation in our code. This step results in a complete dataset, denoted as <span class="math inline">X^{(0)}</span>.</p></li>
<li><p><strong>Iterative Process</strong>: For each feature <span class="math inline">X_j</span> with missing values, the algorithm performs the following steps:</p>
<ul>
<li>Treat <span class="math inline">X_j</span> as a dependent variable and other features <span class="math inline">X_{-j}</span> as independent variables.</li>
<li>Build a regression model using <span class="math inline">X_{-j}</span> to predict <span class="math inline">X_j</span>.</li>
<li>Update the imputed values of <span class="math inline">X_j</span> in the dataset using the model’s predictions.</li>
</ul></li>
<li><p><strong>Repeat</strong>: This process is repeated, cycling through all features with missing values multiple times.</p></li>
</ol>
<p>Each iteration refines the imputations based on the relationships learned from the regression models. The regression models can vary: linear regression is commonly used for continuous data, while logistic regression or other classification models are utilized for categorical data. The choice of regression model significantly influences the imputation quality. We used the default Baysian rifge regression for our code.</p>
<p>In Scikit-learn’s Iterative Imputer, flexibility is provided in terms of the regression estimator (e.g., BayesianRidge, DecisionTreeRegressor), maximum number of imputation rounds (Max Iter), and convergence criterion (Tolerance). The iteration continues until either the change in imputed values across iterations falls below the specified tolerance level or the maximum number of iterations is reached.</p>
<p>In our code, we used BayesianRidge as our regression estimator, a maximum of 100 iterations, and a tolerance of 0.001.</p>
<p>This approach is especially powerful in scenarios where the dataset features complex relationships among variables. Its effectiveness, however, is highly dependent on the appropriate selection of regression models and the careful handling of convergence criteria. Despite being computationally demanding, Iterative Imputer is a valuable tool when simple imputation methods might not be sufficient.</p>
</section>
<section id="comparison-metrics" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="comparison-metrics"><span class="header-section-number">4.2</span> Comparison metrics</h3>
<p>The first measure we used to compare these different imputation (and hence inference) methods was the Relative Mean Squared Error (rMSE) (not to be confused with Root Mean Square Error, the square root of the MSE, hence we denote it rMSE and not RMSE) of the imputed data matrix as compared to the true data matrix (we have access to the true data matrix since we first simulated it and then removed values). This does not directly compare the estimated mean and covariance with the true mean and covariance. However, as stated above, it is a good proxy because we expect the imputation of the EM algorithm to be good exactly when its inference was good, and because the quality of the inference of the imputation methods depends directly on how good their imputation is. In a second step, we will also look at the accuracy of our estimates directly.</p>
<p>We remove varying percentages of our simulated data according to one of the three missing data mechanisms above, and calculate the rMSE of the imputed data, mean, and covariance for that percentage of missingness.</p>
<p>For the mean, for example, denoting <span class="math inline">\mu</span> the true mean, we calculate</p>
<p><span class="math display">
rMSE=
\frac{1/n\sum (\hat{\mu}_i-\mu_i)^2}{1/n\sum\mu_i^2}
=\frac{\Vert \hat{\mu}-\mu \Vert^2}{\Vert \mu \Vert^2}
</span></p>
<p>and the same for the imputed data and covariance with the Frobenius matrix norm.</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<p><img src="report_files/figure-html/cell-4-output-1.png" width="1690" height="2726"></p>
</div>
</div>
<p>For Missing Completely At Random data, we see that the EM algorithm performs best, closely followed by the multiple imputation method. The KNNimputation method seem to capture some of the relationship between the features and as such usually performs better than the median imputation method, which does not capture any relationship between features. Surprisingly, however, the rMSE of the KNN method seems to perform worse than median at higher level of missing data. We also see that the performance declines as the percentage of missing data increases, but not dramatically. This is inevitable as we have less data to work with.</p>
<p>For Missing At Random data, we see that the performance of the EM algorithm is very similar to the graph above. This shows that the EM algorithm can indeed deal with MAR data. We also notice that the Median imputation seem to perform significantly worse in the rMSE of the mean for MAR compared to MCAR data and that the rMSE of the mean for the Iterative method performs worse than KNN at higher level of data missingness, possibly due to the fact that the method stopped by reaching the maximum iteration instead of converging.</p>
<p>While the comparative performance of the different imputation methods do not change much for Missing Not At Random data compared to MAR, the performance of all the methods are worse, especially when looking at rMSE of the mean. This is as expected, because no method can reliably perform inference on MNAR (in generality) without somehow correctly modeling the mechanism which produces the missingness.</p>
</section>
</section>
<section id="the-impact-of-different-types-of-covariance-matrices-on-the-em-algorithm" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="the-impact-of-different-types-of-covariance-matrices-on-the-em-algorithm"><span class="header-section-number">5</span> The impact of different types of covariance matrices on the EM algorithm</h2>
<p>To analyze further the situations in which the EM algorithm performs well or not so well, we repeated this analysis for different types of covariance matrices. The idea is that what the EM algorithm is particularly good at is estimating the covariance matrix, and correctly imputing the data in a Gaussian setting, while for example the MICE method is more general, but potentially less adept in the Gaussian setting at doing this.</p>
<p>For this we generated two types of covariance matrix. First, we generated a “sparse” 5x5 covariance matrix, which contained an important proportion of zeros. Such a matrix was generated by creating a lower triangular Cholesky factor, with lower triangular entries that were either random or had a certain probability of being zero, and then doing the cross-product to obtain the covariance.</p>
<p>For this sparse covariance structure, we see that the EM algorithm does not enjoy the clear performance advantage it did previously. In general, all the methods we compared perform similarly for imputation. In fact, median imputation was often best. This is not too surprising: indeed, consider the most extreme “sparse” covariance: a diagonal matrix. In this case, the components of the Gaussian random vector are independent, and so the MLE for the mean of the i-th component is simply the sample mean of the i-th column of the data matrix. The best prediction we can make for a missing entry in the data matrix is then the mean of its column, ie the mean imputation method, (mean and median are essentially the same for Gaussians with a decent amount of data). Hence in this case, we expect median imputation to perform well or even best for imputation. In theory, the EM algorithm should also be able to do the same thing as median imputation, but as it also has to estimate the covariance matrix and inevitably will make a bit of error there, it can actually be worse. For the first time, we see that the multiple imputation method tends to perform a bit better than EM for all missing data mechanisms.</p>
<div class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<p><img src="report_files/figure-html/cell-6-output-1.png" width="1690" height="2726"></p>
</div>
</div>
<p>Secondly, we wanted to analyze the performance of the EM algorithm for highly correlated multivariate Gaussian data. For this, we used a very specific type of correlation matrix. Suppose you have a 2D grid of points, and each point is assigned a value from a multivariate distribution. For example, a 3x3 grid corresponds to a multivariate 9-dimensional Gaussian distribution. The correlation of points is determined by their euclidean distance. We set the correlation to <span class="math inline">\exp\left(\frac{\Vert x_i-x_j\Vert}{h} \right)</span> with <span class="math inline">h=1.5</span>. The resulting multivariate normal is very correlated.</p>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img src="report_files/figure-html/cell-8-output-1.png" width="1690" height="2726"></p>
</div>
</div>
<p>With this correlation structure, the EM algorithm becomes superior again. However, the results are very similar to just randomly generating a covariance; in hindsight, this makes sense because a randomly generated covariance matrix is going to be highly correlated as well.</p>
</section>
<section id="non-gaussian-distributions" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="non-gaussian-distributions"><span class="header-section-number">6</span> Non-Gaussian Distributions</h2>
<section id="real-data-with-no-distribution-assumption" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="real-data-with-no-distribution-assumption"><span class="header-section-number">6.1</span> Real Data with no distribution assumption</h3>
<p>We now try to perform the same analysis starting from a complete dataset of unknown distribution. In fact, the EM algorithm relies on the assumption that the underlying distribution is known, which is not the case for the other methods we are comparing to here.</p>
<p>We encountered numerical problems in updating the covariance in the EM algorithm in this application. When calculating the conditional covariance which we denoted <span class="math inline">C^{(n)(l-1)}</span> in chapter 2, we split the covariance matrix into four blocks according to the missing and observed data, and we calculate the conditional covariance of the missing data with the formula <span class="math display">
\Sigma_{2.1}=\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}
</span></p>
<p>which is the source of our numerical problems. To counter this, we regularized our covariance at every iteration. If the ratio between the largest and smallest eigenvalue became too big, we added a small fraction of the largest eigenvalue to the diagonal, to decrease the ratio. This seemed to solve our issues. The data set considered is not the source of interest and will not be described, the goal is simply to test the methods on data on which we cannot make any a priori assumption on the distribution. We extracted a subset of this data set about wine quality, considering 300 observations of some variables (fixed acidity, volitile acidity, density and pH). The data set is available at <span class="citation" data-cites="misc_wine_quality_186">(<a href="#ref-misc_wine_quality_186" role="doc-biblioref">Cortez and Reis, 2009</a>)</span>.</p>
<p>We apply the Shapiro-Wilk test for normality which rejected the assumption of normality with a high certainty (the p-value of order <span class="math inline">10^{-35}</span>), as expected. As in the previous scenarios, we imputed the data and made inference on the mean and the covariance matrix using various methods. The results are shown in the following figure.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="report_files/figure-html/cell-9-output-1.png" width="1690" height="2726"></p>
</div>
</div>
<p>Quite surprisingly, the EM algorithm imputes as well as the other methods, despite them not relying on the assumption of Gaussianity (which is not satisfied here).</p>
<p>For the mean estimation, all methods including EM are stable, even for large amounts of missing data. The graphs for the MAR and MNAR mechanisms show much more instability for all methods, and the EM algorithm tends to perform poorly.</p>
<p>For the covariance, we observe that the Iterative method outperforms the others for all missing data mechanisms. While the Median, KNN and Iterative methods seem to convergence to the true covariance, at least for low missing data percentages, the EM algorithm does not at all (the normalized MSE being of order 1, testifying of a wrong convergence). We suspect that this is due to how the covariance is estimated in th EM algorithm for Gaussian data. Recall that the formula to update our covariance estimate in chapter 2 was <span class="math display">
\Sigma^{(l)}=\frac{1}{N}\sum_{n=1}^N(\hat{x}^{(n)(l-1)}-\mu^{(l)})(\hat{x}^{(n)(l-1)}-\mu^{(l)})^T+ C^{(n)(l-1)}
</span></p>
<p>where <span class="math display">
C^{(n)(l-1)}=\mathbb{E}_{\theta^{(l-1)}}\left[(X^{(n)}-\hat{x}^{(n)(l-1)})(X^{(n)}-\hat{x}^{(n)(l-1)})^T |X_{obs}=x_{obs} \right]
</span></p>
<p>was the conditional covariance of the n-th random vector <span class="math inline">X^{(n)}</span>, given that we know <span class="math inline">X_{obs}=x_{obs}</span>. Note that, at convergence, this corresponds to the empirical covariance of the imputed data, plus the term <span class="math inline">C^{(n)(l-1)}</span>. The added term is supposed to account for the fact that, if we simply impute all the terms by their conditional expected value, we end up with less variability than the truth. This is because, if we wanted to simulate the missing values, they would have a normal distribution centered around this mean, and not be exactly equal to it, and this added variability would get eliminated if we ignored <span class="math inline">C^{(n)(l-1)}</span>. <span class="math inline">C^{(n)(l-1)}</span> can be calculated from <span class="math inline">\Sigma^{(l-1)}</span> directly. We suspect that, because this data is not Gaussian, this calculation may not yield something close to the correct conditional covariance, screwing up the results. However, we also cannot completely rule out that our covariance regularization may also be at fault. This could be analyzed further given more time.</p>
</section>
<section id="students-t-data" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="students-t-data"><span class="header-section-number">6.2</span> Student’s-t Data</h3>
<p>The multivariate Student-t distribution is useful for modeling datasets with heavy tails and is often used in finance. Quite often, especially in finance, the assumption of Gaussianity on Student-t data can lead to bad estimations, we will therefore analyse this.</p>
<p>The multivariate Student-t has the following parameters</p>
<ol type="1">
<li><strong>Mean vector</strong> <span class="math inline">\mu</span>: A <span class="math inline">d</span>-dimensional vector representing the mean of the distribution.</li>
<li><strong>Scale matrix</strong> <span class="math inline">\Sigma</span>: A positive definite <span class="math inline">d \times d</span> matrix.</li>
<li><strong>Degrees of freedom</strong> <span class="math inline">\nu &gt; 2</span>: A scalar value that determines the shape of the distribution’s tails. As <span class="math inline">\nu</span> increases, the Student-t distribution approaches the normal distribution.</li>
</ol>
<p>Its density function is given by:</p>
<p><span class="math display">
f(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}, \nu) = \frac{\Gamma\left(\frac{\nu + d}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right) \nu^{\frac{d}{2}} \pi^{\frac{d}{2}} |\boldsymbol{\Sigma}|^{\frac{1}{2}}} \left(1 + \frac{1}{\nu} (\mathbf{x} - \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right)^{-\frac{\nu + d}{2}}
</span></p>
<p>where <span class="math inline">\mathbf{x}</span> is a <span class="math inline">d</span>-dimensional data vector, <span class="math inline">\Gamma</span> is the gamma function, and <span class="math inline">|\Sigma|</span> is the determinant of <span class="math inline">\Sigma</span>. The covariance is given by <span class="math inline">\nu \Sigma / (\nu -2)</span>.</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="report_files/figure-html/cell-10-output-1.png" width="1690" height="2726"></p>
</div>
</div>
<p>For Student’s-t data, EM performs roughly as well as, but not better than, the other methods. Note that computationally, our EM implementation is more demanding.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">7</span> Conclusion</h2>
<p>We investigated the EM-algorithm’s performance across a variety of situations, focusing on its Gaussian implementation. We analyzed its inference capabilities and compared them to doing inference with imputed data, using various imputation algorithms. We also investigated its suitability for imputation. The EM algorithm performed best or near-best across all settings with Gaussian data. It performed best for MCAR data, although inference on MAR data was nearly as reliable. Surprisingly, for the MNAR mechanism we used in our analysis, the EM algorithm still performed quite well, better than some of the other algorithms did on MAR data - but the decline in performance was still bigger than the difference between MCAR and MAR data. Obviously, if the data were MNAR in a particularly “pathological” way, no algorithm would be able to recover the proper parameters (for example, if all values above a certain threshold vanish, we will inevitably underestimate the mean dramatically, unless we know and account for this). Performance is particularly good when the correlation between variables of the Gaussian random vector is high; the advantage of using the EM algorithm for inference on missing data dwindles if the covariance matrix is “sparse”. A major weakness of the EM algorithm is that it makes distributional assumptions, and requires working with the likelihood when being implemented, while other imputation and inference methods (such as multiple imputation) are more general. This was evident in the Gaussian EM’s failure to estimate the covariance matrix for real-life non-Gaussian data. However, if the misspecification is slight, such as when the data stem from a student distribution with many degrees of freedom, the consequences are not severe. EM inference and imputation is hence a valuable tool for dealing with missing data.</p>
</section>
<section id="appendix" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="appendix"><span class="header-section-number">8</span> Appendix</h2>
<p>We also looked at nearly normal data but the result are very similar to the normal data, we therefore do not show them in the main part. This result is shown in the Figure 1 below.</p>
<div id="nearly_normal" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/nearly_normal_results.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Comparison of nearly gaussian data and estimation of parameters across different missingness levels and mechanisms</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-JSSv045i03" class="csl-entry" role="listitem">
Buuren, S. van, and Groothuis-Oudshoorn, K. (2011). Mice: Multivariate imputation by chained equations in r. <em>Journal of Statistical Software</em> 45, 1–67. doi:<a href="https://doi.org/10.18637/jss.v045.i03">10.18637/jss.v045.i03</a>.
</div>
<div id="ref-misc_wine_quality_186" class="csl-entry" role="listitem">
Cortez, C., Paulo, and Reis, J. (2009). <span>Wine Quality</span>.
</div>
<div id="ref-10.1111/j.2517-6161.1977.tb01600.x" class="csl-entry" role="listitem">
Dempster, A. P., Laird, N. M., and Rubin, D. B. (2018). <span class="nocase">Maximum Likelihood from Incomplete Data Via the EM Algorithm</span>. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 39, 1–22. doi:<a href="https://doi.org/10.1111/j.2517-6161.1977.tb01600.x">10.1111/j.2517-6161.1977.tb01600.x</a>.
</div>
<div id="ref-math_517_website" class="csl-entry" role="listitem">
Math 517 Course (2023). Week 6 notes. Available at: <a href="https://math-517.github.io/math_517_website/notes/week_06.html">https://math-517.github.io/math_517_website/notes/week_06.html</a>.
</div>
<div id="ref-rmisstastic_generate_missing_values" class="csl-entry" role="listitem">
Rmisstastic How to generate missing values. Available at: <a href="https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values">https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values</a>.
</div>
<div id="ref-10.1093/bioinformatics/17.6.520" class="csl-entry" role="listitem">
Troyanskaya, O., Cantor, M., Sherlock, G., Brown, P., Hastie, T., Tibshirani, R., et al. (2001). <span class="nocase">Missing value estimation methods for DNA microarrays </span>. <em>Bioinformatics</em> 17, 520–525. doi:<a href="https://doi.org/10.1093/bioinformatics/17.6.520">10.1093/bioinformatics/17.6.520</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>