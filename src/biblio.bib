
@phdthesis{Vitay2017,
  type = {Habilitation},
  title = {On the Role of Dopamine in Motivated Behavior: A Neuro-Computational Approach},
  author = {Vitay, Julien},
  year = {2017},
  address = {{Chemnitz}},
  url = {https://julien-vitay.net/publication/vitay2017/},
  abstract = {Neuro-computational models allow to study the brain mechanisms involved in intelligent behavior and extract essential computational principles which can be implemented in cognitive systems. They are a promising solution to achieve a brain-like artificial intelligence that can compete with natural intelligence on realistic behaviors. A crucial property of intelligent behavior is motivation, defined as the incentive to interact with the world in order to achieve specific goals, either extrinsic (obtaining rewards such as food or money, or avoiding pain) or intrinsic (satisfying one's curiosity, fun). In the human brain, motivated or goal-directed behavior depends on a network of different structures, including the prefrontal cortex, the basal ganglia and the limbic system. Dopamine, a neuro-transmitter associated with reward processing, plays a central role in coordinating the activity of this network. It structures processing in high-level cognitive areas along a limbic-associative-motor gradient and impacts the learning capabilities of the whole system. In this habilitation thesis, I present biologically-constrained neuro-computational models which investigate the role of dopamine in visual object categorization and memory retrieval (Vitay and Hamker, 2008), reinforcement learning and action selection (Vitay and Hamker, 2010), the updating, learning and maintenance of working memory (Schroll, Vitay and Hamker, 2012) and timing processes (Vitay and Hamker, 2014). These models outline the many mechanisms by which the dopaminergic system regulates cognitive and emotional behavior: bistable processing modes in the cerebral cortex, modulation of synaptic transmission and plasticity, allocation of cognitive resources and signaling of relevant events. Finally, I present a neural simulator able to simulate a variety of neuro-computational models efficiently on parallel architectures (Vitay, Dinkelbach and Hamker, 2015).},
  school = {Technische Universit\"at Chemnitz}
}

@article{10.1093/bioinformatics/17.6.520,
    author = {Troyanskaya, Olga and Cantor, Michael and Sherlock, Gavin and Brown, Pat and Hastie, Trevor and Tibshirani, Robert and Botstein, David and Altman, Russ B.},
    title = "{Missing value estimation methods for DNA microarrays }",
    journal = {Bioinformatics},
    volume = {17},
    number = {6},
    pages = {520-525},
    year = {2001},
    month = {06},
    abstract = "{Motivation: Gene expression microarray experiments can generate
  data sets with multiple missing expression values. Unfortunately,
  many algorithms for gene expression analysis require a complete
  matrix of gene array values as input. For example, methods such as
  hierarchical clustering and K-means clustering are not robust to
  missing data, and may lose effectiveness even with a few missing
  values. Methods for imputing missing data are needed, therefore, to
  minimize the effect of incomplete data sets on analyses, and to
  increase the range of data sets to which these algorithms can be
  applied. In this report, we investigate automated methods for
  estimating missing data.Results: We present a comparative study of several methods for
  the estimation of missing values in gene microarray data. We
  implemented and evaluated three methods: a Singular Value
  Decomposition (SVD) based method (SVDimpute), weighted K-nearest
  neighbors (KNNimpute), and row average. We evaluated the methods
  using a variety of parameter settings and over different real data
  sets, and assessed the robustness of the imputation methods to the
  amount of missing data over the range of 1–20\\% missing values.
  We show that KNNimpute appears to provide a more robust and
  sensitive method for missing value estimation than SVDimpute, and
  both SVDimpute and KNNimpute surpass the commonly used row average
  method (as well as filling missing values with zeros). We report
  results of the comparative experiments and provide recommendations
  and tools for accurate estimation of missing microarray data under a
  variety of conditions.Availability: The software is available at http://smi-web.stanford.edu/projects/helix/pubs/impute/Contact: russ.altman@stanford.edu*To whom correspondence should be
  addressed.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/17.6.520},
    url = {https://doi.org/10.1093/bioinformatics/17.6.520},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/17/6/520/48837104/bioinformatics\_17\_6\_520.pdf},
}


@article{JSSv045i03,
 title={mice: Multivariate Imputation by Chained Equations in R},
 volume={45},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v045i03},
 doi={10.18637/jss.v045.i03},
 abstract={The R package &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt;, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
 number={3},
 journal={Journal of Statistical Software},
 author={van Buuren, Stef and Groothuis-Oudshoorn, Karin},
 year={2011},
 pages={1–67}
}

@article{10.1111/j.2517-6161.1977.tb01600.x,
    author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
    title = "{Maximum Likelihood from Incomplete Data Via the EM Algorithm}",
    journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
    volume = {39},
    number = {1},
    pages = {1-22},
    year = {2018},
    month = {12},
    abstract = "{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}",
    issn = {0035-9246},
    doi = {10.1111/j.2517-6161.1977.tb01600.x},
    url = {https://doi.org/10.1111/j.2517-6161.1977.tb01600.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/39/1/1/49117094/jrsssb\_39\_1\_1.pdf},
}


@online{rmisstastic_generate_missing_values,
  author = {{Rmisstastic}},
  title = {How to Generate Missing Values},
  url = {https://rmisstastic.netlify.app/how-to/python/generate_html/how%20to%20generate%20missing%20values},
  note = {Accessed: 2023-12-23}
}

@misc{misc_wine_quality_186,
  author       = {Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.},
  title        = {{Wine Quality}},
  year         = {2009},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C56S3T}
}

@online{math_517_website,
  author = {{Math 517 Course}},
  title = {Week 6 Notes},
  year = {2023},
  url = {https://math-517.github.io/math_517_website/notes/week_06.html},
  note = {Accessed: 2023-12-23}
}
