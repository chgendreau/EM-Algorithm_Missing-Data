{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c39df80",
   "metadata": {},
   "source": [
    "# EM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "004a7479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.3983968  -2.06094046  1.89586593]\n",
      " [ 2.69711312 -2.68829015  3.56506145]\n",
      " [ 3.30992619 -1.69128094  3.94958089]\n",
      " ...\n",
      " [ 1.87525438 -2.20035152  2.28319017]\n",
      " [ 1.35402411 -1.65843653  2.56047618]\n",
      " [-0.92871722 -3.69263405 -0.22865864]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define mean and covariance matrix for a multivariate Gaussian distribution\n",
    "mean = [1, -3, 2]\n",
    "cov = [[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]]\n",
    "\n",
    "# Generate the data\n",
    "data = np.random.multivariate_normal(mean, cov, size=1000)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7512cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Data with Missing Values:\n",
      " [[ 2.3983968          nan  1.89586593]\n",
      " [ 2.69711312 -2.68829015  3.56506145]\n",
      " [ 3.30992619 -1.69128094  3.94958089]\n",
      " ...\n",
      " [        nan -2.81750546  0.88582774]\n",
      " [ 0.72765928 -3.33800423         nan]\n",
      " [ 2.459435   -2.97771047         nan]]\n"
     ]
    }
   ],
   "source": [
    "# Introduce missing data (NaN)\n",
    "missing_rate = 0.2  # 20% of the data is missing\n",
    "mask = np.random.rand(*data.shape) < missing_rate\n",
    "data[mask] = np.nan\n",
    "print(\"Generated Data with Missing Values:\\n\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a15ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_covariance(data): #not sure how to initialise without a function like this, np.cov() does not work well as there is alot of NaN in the matrix\n",
    "    n, m = data.shape # this function basicaly computes covariance ignoring the mssiing data\n",
    "    mean_cols = np.nanmean(data, axis=0)\n",
    "    cov_matrix = np.empty((m, m))\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            valid_data = ~np.isnan(data[:, i]) & ~np.isnan(data[:, j])\n",
    "            centered_i = data[valid_data, i] - mean_cols[i]\n",
    "            centered_j = data[valid_data, j] - mean_cols[j]\n",
    "            cov_matrix[i, j] = np.mean(centered_i * centered_j)\n",
    "\n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feaea8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.3983968  -3.00348868  1.89586593]\n",
      " [ 2.69711312 -2.68829015  3.56506145]\n",
      " [ 3.30992619 -1.69128094  3.94958089]\n",
      " ...\n",
      " [ 0.98703278 -2.81750546  0.88582774]\n",
      " [ 0.72765928 -3.33800423  1.98224057]\n",
      " [ 2.459435   -2.97771047  1.98224057]]\n"
     ]
    }
   ],
   "source": [
    "#Nans in data are replaced by column mean\n",
    "imputed_data = np.where(np.isnan(data), np.nanmean(data, axis=0), data)\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be10e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood1(X, mu, Sigma):\n",
    "    N=X.shape[0]\n",
    "    p=X.shape[1]\n",
    "    result=-N/2*np.log(np.linalg.det(Sigma))\n",
    "    X_muT=X-mu.T\n",
    "    for n in range(N):\n",
    "        result=result-0.5*(X[n,:]-mu.T).dot(np.linalg.solve(Sigma, X[n,:].T-mu)) #is repetitively summing here bad?\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2609b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_xhat(xn,mu,Sigma,missing_indices):\n",
    "    #m=missing_indices.shape[0]\n",
    "    p=Sigma.shape[0]\n",
    "    \n",
    "    xhatn=xn.copy()\n",
    "    observed_indices=np.arange(p)[~np.isin(np.arange(p),missing_indices)]\n",
    "    #appended_indices=np.append(observed_indices, missing_indices)\n",
    "    \n",
    "    Sigma22=Sigma[missing_indices,:][:,missing_indices]\n",
    "    Sigma11=Sigma[observed_indices,:][:,observed_indices]\n",
    "    Sigma21=Sigma[missing_indices,:][:,observed_indices]\n",
    "    \n",
    "    mu1=mu[observed_indices]\n",
    "    mu2=mu[missing_indices]\n",
    "    xhatn_1=xhatn[observed_indices]\n",
    "    mu2_conditional=mu2+Sigma21.dot(np.linalg.solve(Sigma11,xhatn_1-mu1))\n",
    "    #print(xhatn)\n",
    "    #print(mu2_conditional,xhatn[missing_indices,],mu2_conditional.shape,xhatn[missing_indices,].shape)\n",
    "    xhatn[missing_indices,] = mu2_conditional\n",
    "    #this doesnt seem to work \n",
    "    #reason: Array is passed as a view, potential danger!\n",
    "    #hmm still doesnt work wtf\n",
    "    #Its because a numpy array of ints rounds floats that are added to it wtfffff, fixed now\n",
    "    \n",
    "    #print(xhatn.base)\n",
    "    #print(mu2_conditional,xhatn[missing_indices])\n",
    "    \n",
    "    return xhatn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e72d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C(Sigma,missing_indices,verbose=False):\n",
    "    p=Sigma.shape[0]\n",
    "    m=missing_indices.shape[0]\n",
    "    if verbose:\n",
    "        print(\"Original unconditional covariance : \\n\",Sigma)\n",
    "    observed_indices=np.arange(p)[~np.isin(np.arange(p),missing_indices)]\n",
    "    appended_indices=np.append(observed_indices, missing_indices)\n",
    "    Sigma=(Sigma[appended_indices,:])[:,appended_indices]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Unconditional covariance rearranged so that missing indices are at the bottom : \\n\",Sigma)\n",
    "    Sigma11=Sigma[0:(p-m),0:(p-m)]\n",
    "    Sigma21=Sigma[(p-m):p,0:(p-m)]\n",
    "    Sigma22=Sigma[(p-m):p,(p-m):p]\n",
    "    Sigma22_conditional=Sigma22-Sigma21.dot(np.linalg.solve(Sigma11,Sigma21.T))\n",
    "    \n",
    "    reverse_permutation=np.argsort(appended_indices)\n",
    "    result=np.full((p,p),0)\n",
    "    result[(p-m):p,(p-m):p]=Sigma22_conditional\n",
    "    if verbose:\n",
    "        print(\"The conditional covariance when ordered is : \\n\",result)\n",
    "    result=(result[reverse_permutation,:])[:,reverse_permutation]\n",
    "    if verbose:\n",
    "        print(\"After permuting back we get : \\n\", result)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce156675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_algorithm1(data, max_iter=100, tol=1e-6):\n",
    "    n, p = data.shape\n",
    "\n",
    "    # Initialize mean and covariance estimates\n",
    "    means = np.nanmean(data, axis=0)\n",
    "    covariance = nan_covariance(data)\n",
    "\n",
    "    # Create an array to hold imputed data\n",
    "    imputed_data = np.where(np.isnan(data), np.nanmean(data, axis=0), data)\n",
    "    #imputed_data = np.where(np.isnan(data), None, data)\n",
    "    \n",
    "    old_log_likelihood = log_likelihood1(imputed_data, means, covariance)\n",
    "    #print(old_log_likelihood)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        # E-step: Estimate missing values\n",
    "        for i in range(n):\n",
    "            missing = np.where(np.isnan(data[i]))[0]\n",
    "            if np.isnan(data[i]).any():\n",
    "                imputed_data[i,] = calculate_xhat(imputed_data[i,],means,covariance,missing) #this function works with Nans in the missing entries too, why do we impute?\n",
    "\n",
    "        # M-step: Update mean and covariance estimates\n",
    "        means = np.mean(imputed_data, axis=0)\n",
    "        new_covariance = np.full((p,p),0) \n",
    "\n",
    "        # Add the conditional covariance for missing data\n",
    "        for i in range(n):\n",
    "            missing = np.where(np.isnan(data[i]))[0]\n",
    "            #if missing.any(): #with this condition, are you not skipping xhat-mu *(xhat-mu).T when there is no missing data? I think so...\n",
    "            new_covariance = new_covariance + calculate_C(covariance,missing,verbose=False) + (imputed_data[i,]-means).reshape(-1, 1) @ np.transpose((imputed_data[i,]-means).reshape(-1, 1))\n",
    "            \n",
    "        covariance = new_covariance/n \n",
    "        #print(covariance)\n",
    "        # Convergence test based on log likelihood\n",
    "        new_log_likelihood = log_likelihood1(imputed_data, means, covariance)\n",
    "        #print(new_log_likelihood)\n",
    "        difference=new_log_likelihood - old_log_likelihood\n",
    "        print(\"The new log likelihood is :\", new_log_likelihood, \"  Difference of : \",difference)\n",
    "        \n",
    "        if np.abs(difference) < tol: #absolute value not necessary and potentially undesirable, as in theory should always be positive\n",
    "            print(\"Convergence achieved! \\n\")\n",
    "            break\n",
    "        old_log_likelihood = new_log_likelihood\n",
    "\n",
    "    return imputed_data,means,covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c715472c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new log likelihood is : -9559.935349729634   Difference of :  1041.9468425925152\n",
      "The new log likelihood is : -9543.07634579237   Difference of :  16.859003937264788\n",
      "The new log likelihood is : -9541.805904219213   Difference of :  1.2704415731568588\n",
      "The new log likelihood is : -9541.67855121083   Difference of :  0.12735300838176045\n",
      "The new log likelihood is : -9541.659193707232   Difference of :  0.019357503599167103\n",
      "The new log likelihood is : -9541.65551960928   Difference of :  0.003674097952170996\n",
      "The new log likelihood is : -9541.654769638795   Difference of :  0.0007499704843212385\n",
      "The new log likelihood is : -9541.654613505236   Difference of :  0.00015613355935784057\n",
      "The new log likelihood is : -9541.654580833263   Difference of :  3.267197280365508e-05\n",
      "The new log likelihood is : -9541.654573987114   Difference of :  6.8461486080195755e-06\n",
      "The new log likelihood is : -9541.65457255199   Difference of :  1.4351244317367673e-06\n",
      "The new log likelihood is : -9541.65457225118   Difference of :  3.008099156431854e-07\n",
      "[ 2.69711312 -2.68829015  3.56506145]\n",
      "Imputed Data:\n",
      " [[ 2.3983968  -2.31454974  1.89586593]\n",
      " [ 2.69711312 -2.68829015  3.56506145]\n",
      " [ 3.30992619 -1.69128094  3.94958089]\n",
      " ...\n",
      " [ 1.06423927 -2.81750546  0.88582774]\n",
      " [ 0.72765928 -3.33800423  1.85108252]\n",
      " [ 2.459435   -2.97771047  2.05109595]] \n",
      "\n",
      "Difference with Old Data with mean imputation \n",
      " [[ 0.          0.68893894  0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " ...\n",
      " [ 0.07720648  0.          0.        ]\n",
      " [ 0.          0.         -0.13115805]\n",
      " [ 0.          0.          0.06885538]]\n"
     ]
    }
   ],
   "source": [
    "imputed_data1,mu,Sigma = em_algorithm1(data) #I don't like the name imputed_data1 for the EM-algo predictions of missing data\n",
    "print(\"Imputed Data:\\n\", imputed_data1, \"\\n\\nDifference with Old Data with mean imputation \\n\",imputed_data1-imputed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b40cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n",
      "[59 83  4]\n",
      "[0 1 0]\n",
      "[ 0.99312482 -3.00423652  1.98251282]\n",
      "[[0.83906014 0.47711016 0.20636147]\n",
      " [0.47711016 0.82627886 0.31812695]\n",
      " [0.20636147 0.31812695 0.84420618]]\n",
      "[[1, 0.5, 0.2], [0.5, 1, 0.3], [0.2, 0.3, 1]]\n"
     ]
    }
   ],
   "source": [
    "difference=imputed_data1-imputed_data #should not be incredibly large\n",
    "\n",
    "print(sum(np.isnan(difference))) # No nans, good\n",
    "print(sum(difference>1))\n",
    "print(sum(difference>2)) #plausible enough\n",
    "print(mu) #estimate is very accurate\n",
    "print(Sigma)#estimate is ok but not superrr precise, especially the ones on the diagonal for example are only about 0.8\n",
    "print(cov) #true covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aee572",
   "metadata": {},
   "source": [
    "# Trying out different patterns of randomness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b41d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e5373ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from produce_NA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c70539da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "\n",
    "# Generating missing data with diffent mechanisms\n",
    "p_miss = 0.2 # missing rate\n",
    "print(type(X))\n",
    "\n",
    "#MCAR\n",
    "X_mcar = produce_NA(X, p_miss=p_miss, mecha='MCAR')['X_incomp'] #So apparently this takes a numpy array but outputs sth else that must be converted\n",
    "print(type(X_mcar))\n",
    "#convert to np.ndarray\n",
    "X_mcar = X_mcar.numpy()\n",
    "\n",
    "#MAR, opt = \"logistic\", \"quantile\", \"selfmasked\"\n",
    "p_obs = 0.5 # proportion of observed data \n",
    "X_mar = produce_NA(X, p_miss=p_miss, mecha='MAR', p_obs=p_obs, opt = \"quantile\")['X_incomp']\n",
    "X_mar = X_mar.numpy()\n",
    "\n",
    "#MNAR, opt = \"logistic\", \"quantile\", \"selfmasked\"\n",
    "q = 0.3 # quantile at which cuts should be made\n",
    "X_mnar = produce_NA(X, p_miss=p_miss, mecha='MNAR',p_obs=p_obs, q=q, opt = \"logistic\")['X_incomp']\n",
    "#convert to np.ndarray\n",
    "X_mnar = X_mnar.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e9cb65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new log likelihood is : -906.4438067267813   Difference of :  101.90603451669654\n",
      "The new log likelihood is : -900.7397138729945   Difference of :  5.704092853786847\n",
      "The new log likelihood is : -899.6979859768948   Difference of :  1.041727896099701\n",
      "The new log likelihood is : -899.4782444518106   Difference of :  0.2197415250841459\n",
      "The new log likelihood is : -899.4263951808666   Difference of :  0.05184927094398972\n",
      "The new log likelihood is : -899.4135689788073   Difference of :  0.012826202059272873\n",
      "The new log likelihood is : -899.410342919679   Difference of :  0.0032260591283375106\n",
      "The new log likelihood is : -899.4095270448003   Difference of :  0.0008158748787536751\n",
      "The new log likelihood is : -899.4093203495108   Difference of :  0.00020669528942107718\n",
      "The new log likelihood is : -899.4092679573357   Difference of :  5.2392175120985485e-05\n",
      "The new log likelihood is : -899.4092546753071   Difference of :  1.3282028589856054e-05\n",
      "The new log likelihood is : -899.409251308063   Difference of :  3.3672441759335925e-06\n",
      "The new log likelihood is : -899.4092504544033   Difference of :  8.53659685162711e-07\n",
      "Convergence achieved! \n",
      "\n",
      "The new log likelihood is : -1016.5544725870978   Difference of :  70.07946668545367\n",
      "The new log likelihood is : -1014.300111013753   Difference of :  2.254361573344795\n",
      "The new log likelihood is : -1014.0178473163268   Difference of :  0.28226369742628776\n",
      "The new log likelihood is : -1013.9807905611806   Difference of :  0.037056755146181786\n",
      "The new log likelihood is : -1013.9756103551767   Difference of :  0.005180206003842613\n",
      "The new log likelihood is : -1013.9748246491465   Difference of :  0.000785706030228539\n",
      "The new log likelihood is : -1013.9746946155923   Difference of :  0.00013003355422824825\n",
      "The new log likelihood is : -1013.9746713437269   Difference of :  2.327186541606352e-05\n",
      "The new log likelihood is : -1013.9746669201247   Difference of :  4.423602149472572e-06\n",
      "The new log likelihood is : -1013.974666043903   Difference of :  8.762217476032674e-07\n",
      "Convergence achieved! \n",
      "\n",
      "The new log likelihood is : -892.7456988895289   Difference of :  104.95994727434527\n",
      "The new log likelihood is : -886.8565987575914   Difference of :  5.889100131937425\n",
      "The new log likelihood is : -885.801122561202   Difference of :  1.0554761963894634\n",
      "The new log likelihood is : -885.5936343733041   Difference of :  0.20748818789786583\n",
      "The new log likelihood is : -885.5454911305683   Difference of :  0.0481432427358186\n",
      "The new log likelihood is : -885.5322380491095   Difference of :  0.0132530814587426\n",
      "The new log likelihood is : -885.5280946747513   Difference of :  0.004143374358250185\n",
      "The new log likelihood is : -885.5266999718488   Difference of :  0.0013947029025302982\n",
      "The new log likelihood is : -885.5262129057218   Difference of :  0.00048706612699334073\n",
      "The new log likelihood is : -885.5260399075215   Difference of :  0.00017299820024163637\n",
      "The new log likelihood is : -885.5259779983695   Difference of :  6.190915200932068e-05\n",
      "The new log likelihood is : -885.5259557706735   Difference of :  2.222769603577035e-05\n",
      "The new log likelihood is : -885.5259477786623   Difference of :  7.992011205715244e-06\n",
      "The new log likelihood is : -885.5259449033186   Difference of :  2.875343625419191e-06\n",
      "The new log likelihood is : -885.5259438685506   Difference of :  1.0347680472477805e-06\n",
      "The new log likelihood is : -885.5259434961188   Difference of :  3.724318275999394e-07\n",
      "Convergence achieved! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputed_X_mcar,mu_mcar,Sigma_mcar=em_algorithm1(X_mcar)\n",
    "\n",
    "imputed_X_mar,mu_mar,Sigma_mar=em_algorithm1(X_mar)\n",
    "\n",
    "imputed_X_mnar,mu_mnar,Sigma_mnar=em_algorithm1(X_mnar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7520860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Values are :  [1, -3, 2] \n",
      " [[1.  0.5 0.2]\n",
      " [0.5 1.  0.3]\n",
      " [0.2 0.3 1. ]]\n",
      "MCAR estimates are :\n",
      " [ 0.96373355 -3.00798158  2.01511967] \n",
      "\n",
      " [[0.80589378 0.42860799 0.13025841]\n",
      " [0.42860799 0.7556942  0.31719239]\n",
      " [0.13025841 0.31719239 0.84486499]]\n",
      "MAR estimates are :\n",
      " [ 0.9877899  -3.02164446  1.99615656] \n",
      "\n",
      " [[0.93824555 0.41939572 0.11437014]\n",
      " [0.41939572 0.75402204 0.25841325]\n",
      " [0.11437014 0.25841325 0.80146878]]\n",
      "MNAR estimates are :\n",
      " [ 0.99961654 -3.01405159  1.97042697] \n",
      "\n",
      " [[0.7930495  0.42466118 0.20684633]\n",
      " [0.42466118 0.76128471 0.33522432]\n",
      " [0.20684633 0.33522432 0.8393951 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"True Values are :\\n\", mean, \"\\n\\n\",np.array(cov))\n",
    "print(\"\\nMCAR estimates are :\\n\",mu_mcar,\"\\n\\n\",Sigma_mcar)\n",
    "print(\"\\nMAR estimates are :\\n\",mu_mar,\"\\n\\n\",Sigma_mar)\n",
    "print(\"\\nMNAR estimates are :\\n\",mu_mnar,\"\\n\\n\",Sigma_mnar) #It even works for MNAR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
